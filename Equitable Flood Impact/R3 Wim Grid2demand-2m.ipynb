{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b6adaaa-232d-48f0-90bd-b321e6212df3",
   "metadata": {},
   "source": [
    "### Calculating the Demand using Grid2demand Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b41264-e8bd-410a-ace6-c908d1235599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid2demand, version 0.3.9\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from grid2demand import GRID2DEMAND\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from shapely.geometry import Point, LineString\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01cbc2f-221d-41c2-8def-db5f5a0e8c67",
   "metadata": {},
   "source": [
    "### Calculate Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f253523-dc2c-4405-9854-c3c318f550e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  : Checking input directory...\n",
      "  : input dir ./data/2m, traverse files by type: csv\n",
      "  : Error: Required files are not satisfied,           missing files are: ['zone.csv']\n",
      "  : Input directory is valid.\n",
      "\n",
      "  : Loading default package settings...\n",
      "  : Package settings loaded successfully.\n",
      "\n",
      "INFO Begin to run function: read_network …\n",
      "  : input dir ./data/2m, traverse files by type: csv\n",
      "INFO Begin to run function: read_node …\n",
      "  : Parallel creating Nodes using Pool with 12 CPUs. Please wait...\n",
      "  : Reading node.csv with specified columns: ['node_id', 'x_coord', 'y_coord', 'activity_type', 'is_boundary', 'poi_id']                 \n",
      "    and chunksize 10000 for iterations...\n",
      "  : Successfully loaded node.csv: 40351 Nodes loaded.\n",
      "INFO Finished running function: read_node, total: 3s\n",
      "\n",
      "INFO Begin to run function: read_poi …\n",
      "  : Reading poi.csv with specified columns: ['poi_id', 'building', 'centroid', 'area', 'geometry']                 \n",
      "    and chunksize 10000 for iterations...\n",
      "  : Parallel creating POIs using Pool with 12 CPUs. Please wait...\n",
      "  : Successfully loaded poi.csv: 34402 POIs loaded.\n",
      "INFO Finished running function: read_poi, total: 2s\n",
      "\n",
      "  : Successfully loaded node.csv and poi.csv: 40351 Nodes and 34402 POIs.\n",
      "INFO Finished running function: read_network, total: 5s\n",
      "\n",
      "  : Note: This method will generate grid-based zones from node_dict.               \n",
      "  : If you want to use your own zones(TAZs),               \n",
      "  : please skip this method and use taz2zone() instead. \n",
      "\n",
      "  : Generating zone dictionary...\n",
      "INFO Begin to run function: net2zone …\n",
      "INFO Begin to run function: get_lng_lat_min_max …\n",
      "INFO Finished running function: get_lng_lat_min_max, total: 0s\n",
      "\n",
      "  : Successfully generated zone dictionary: 400 Zones generated,         \n",
      "    plus 80 boundary gates (points))\n",
      "INFO Finished running function: net2zone, total: 0s\n",
      "\n",
      "  : Synchronizing geometry between zone and node/poi...\n",
      "INFO Begin to run function: sync_zone_and_node_geometry …\n",
      "  : Parallel synchronizing Nodes and Zones using Pool with 12 CPUs. Please wait...\n",
      "  : Successfully synchronized zone and node geometry\n",
      "INFO Finished running function: sync_zone_and_node_geometry, total: 15s\n",
      "\n",
      "INFO Begin to run function: sync_zone_and_poi_geometry …\n",
      "  : Parallel synchronizing POIs and Zones using Pool with 12 CPUs. Please wait...\n",
      "  : Successfully synchronized zone and poi geometry\n",
      "INFO Finished running function: sync_zone_and_poi_geometry, total: 16s\n",
      "\n",
      "No trip rate file is provided, use default trip rate.\n",
      "  : Successfully generated poi trip rate with default setting.\n",
      "  : Successfully generated production and attraction for each node based on poi trip rate.\n",
      "  : Successfully calculated zone production and attraction based on node production and attraction.\n",
      "INFO Begin to run function: calc_zone_od_matrix …\n",
      "  : Parallel calculating zone-to-zone distance matrix using Pool with 12 CPUs. Please wait...\n",
      "  : Successfully calculated zone-to-zone distance matrix\n",
      "INFO Finished running function: calc_zone_od_matrix, total: 19s\n",
      "\n",
      "  : Successfully calculated zone od friction attraction.\n",
      "  : Successfully run gravity model to generate demand.csv.\n",
      "  : Successfully generated agent-based demand data.\n",
      "{'required_files': ['node.csv', 'poi.csv'], 'optional_files': ['zone.csv'], 'node_required_fields': ['node_id', 'x_coord', 'y_coord', 'activity_type', 'is_boundary', 'poi_id'], 'poi_required_fields': ['poi_id', 'building', 'centroid', 'area', 'geometry'], 'zone_required_fields': ['zone_id', 'geometry'], 'data_chunk_size': 10000, 'set_cpu_cores': 12, 'trip_purpose_dict': {1: {'name': 'home-based-work', 'alpha': 28507, 'beta': -0.02, 'gamma': -0.123}, 2: {'name': 'home-based-other', 'alpha': 139173, 'beta': -1.285, 'gamma': -0.094}, 3: {'name': 'non-home-based', 'alpha': 219113, 'beta': -1.332, 'gamma': -0.1}}, 'poi_purpose_prod_dict': {'library': {1: 8.16}, 'university': {1: 1.17}, 'office': {1: 2.04}, 'arts_centre': {1: 0.18}, 'university;yes': {1: 1.17}, 'bank': {1: 12.13}, 'childcare': {1: 11.12}, 'school': {1: 2.04}, 'public': {1: 4.79}, 'post_office': {1: 11.21}, 'pharmacy': {1: 10.29}, 'yes': {1: 1.15}}, 'poi_purpose_attr_dict': {'parking': {1: 2.39}, 'apartments': {1: 0.48}, 'motorcycle_parking': {1: 2.39}, 'theatre': {1: 6.17}, 'restaurant': {1: 7.8}, 'cafe': {1: 36.31}, 'bar': {1: 7.8}, 'bicycle_parking': {1: 2.39}, 'residential': {1: 0.48}, 'commercial': {1: 3.81}, 'house': {1: 0.48}, 'stadium': {1: 0.47}, 'retail': {1: 6.84}, 'fast_food': {1: 14.13}, 'yes': {1: 1.15}}}\n",
      "  : Successfully saved demand.csv to ./data/2m\n",
      "  : Successfully saved agent.csv to ./data/2m\n",
      "  : Successfully saved zone.csv to ./data/2m\n",
      "  : Successfully saved zone_od_dist_table.csv to ./data/2m\n",
      "  : Successfully saved zone_od_dist_matrix.csv to ./data/2m\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"./data/2m\"\n",
    "gd = GRID2DEMAND(input_dir)\n",
    "\n",
    "node_dict, poi_dict = gd.load_network.values()\n",
    "\n",
    "zone_dict = gd.net2zone(node_dict, num_x_blocks=20,num_y_blocks=20)\n",
    "\n",
    "# Generate zone based on grid size with 10 km width and 10km height for each zone\n",
    "# zone_dict = gd.net2zone(node_dict, cell_width=10, cell_height=10, unit=\"km\")\n",
    "\n",
    "# if you have your own zone.csv(TAZs), we can generate zones from your personal TAZs\n",
    "# zone_dict = gd.taz2zone()\n",
    "\n",
    "\n",
    "# Synchronize geometry info between zone, node and poi\n",
    "#       add zone_id to node and poi dictionaries\n",
    "#       also add node_list and poi_list to zone dictionary\n",
    "updated_dict = gd.sync_geometry_between_zone_and_node_poi(zone_dict, node_dict, poi_dict)\n",
    "\n",
    "zone_dict_update, node_dict_update, poi_dict_update = updated_dict.values()\n",
    "\n",
    "# Generate poi trip rate for each poi\n",
    "\n",
    "poi_trip_rate = gd.gen_poi_trip_rate(poi_dict_update, trip_rate_file=\"\", trip_purpose=1)\n",
    "\n",
    "# Generate node production attraction for each node based on poi_trip_rate\n",
    "\n",
    "node_prod_attr = gd.gen_node_prod_attr(node_dict_update, poi_trip_rate)\n",
    "\n",
    "# Calculate zone production and attraction based on node production and attraction\n",
    "\n",
    "zone_prod_attr = gd.calc_zone_prod_attr(node_prod_attr, zone_dict_update)\n",
    "\n",
    "# Calculate zone-to-zone od distance matrix\n",
    "\n",
    "zone_od_distance_matrix = gd.calc_zone_od_distance_matrix(zone_dict_update)\n",
    "\n",
    "# Run gravity model to generate agent-based demand\n",
    "\n",
    "df_demand = gd.run_gravity_model(zone_prod_attr, zone_od_distance_matrix)\n",
    "# generate agent-based demand\n",
    "df_agent = gd.gen_agent_based_demand(node_prod_attr, zone_prod_attr, df_demand=df_demand)\n",
    "\n",
    "print(gd.pkg_settings)\n",
    "\n",
    "# Output demand, agent, zone, zone_od_dist_table, zone_od_dist_matrix files\n",
    "gd.save_demand\n",
    "gd.save_agent\n",
    "gd.save_zone\n",
    "gd.save_zone_od_dist_table\n",
    "gd.save_zone_od_dist_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3dee08-843d-4a89-b87c-d614d27c6912",
   "metadata": {},
   "source": [
    "#### Finding the closest node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736eca9d-903b-4eec-8833-0301264ea623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest node is now located in the folder\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate haversine distance\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Load the zone and node CSVs into dataframes\n",
    "zones_df = pd.read_csv('./data/2m/zone.csv', converters={'node_id_list': literal_eval}, low_memory=False )\n",
    "nodes_df = pd.read_csv('./data/2m/node.csv',low_memory=False)\n",
    "\n",
    "# Function to find the closest node ID to the zone's centroid where the node is a POI\n",
    "def find_closest_node_id(node_id_list, centroid_x, centroid_y):\n",
    "    min_distance = float('inf')\n",
    "    closest_node_id = None\n",
    "    poi_nodes_df = nodes_df[nodes_df['activity_type'] == 'poi']  # Filter nodes for POIs only\n",
    "    for node_id in node_id_list:\n",
    "        node = poi_nodes_df[poi_nodes_df['node_id'] == node_id]\n",
    "        if not node.empty:\n",
    "            distance = haversine(centroid_x, centroid_y, node.iloc[0]['x_coord'], node.iloc[0]['y_coord'])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_node_id = node_id\n",
    "    return closest_node_id\n",
    "\n",
    "# Add a new column for the closest node ID to each zone where production > 0\n",
    "zones_df['closest_node_id'] = zones_df.apply(\n",
    "    lambda row: find_closest_node_id(row['node_id_list'], row['centroid_x'], row['centroid_y']) if row['production'] > 0 else None, axis=1)\n",
    "\n",
    "# Output the zones dataframe with the closest node ID\n",
    "zones_df.to_csv('./data/2m/updated_zones_with_closest_node.csv', index=False)\n",
    "\n",
    "centroid_nodes_df = nodes_df[nodes_df['node_id'].isin(zones_df['closest_node_id'])]\n",
    "\n",
    "# Export this filtered DataFrame to a new CSV file\n",
    "centroid_nodes_df.to_csv('./data/2m/centroid_nodes.csv', index=False)\n",
    "\n",
    "print( \"Closest node is now located in the folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05836ba5-39f6-4006-a2a4-259f5eefaa67",
   "metadata": {},
   "source": [
    "#### Strip the Unsused POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb95966-2856-42e9-96a8-6a91f5fdac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used POI and Connector are now stripped out\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV files\n",
    "nodes = pd.read_csv('./data/2m/node.csv', low_memory=False)\n",
    "links = pd.read_csv('./data/2m/link.csv', low_memory=False)\n",
    "centroid_nodes = pd.read_csv('./data/0.2m/centroid_nodes.csv')\n",
    "\n",
    "# Convert node_id in centroid_nodes to a set for fast lookup\n",
    "centroid_node_ids = set(centroid_nodes['node_id'])\n",
    "\n",
    "# Filter nodes to keep\n",
    "nodes_filtered = nodes[(nodes['node_id'].isin(centroid_node_ids)) | (nodes['activity_type'] != 'poi')]\n",
    "\n",
    "nodes_filtered.to_csv('./data/2m/node.csv', index=False)\n",
    "\n",
    "# Update the set of valid node_ids from the filtered nodes\n",
    "valid_node_ids = set(nodes_filtered['node_id'])\n",
    "\n",
    "# Filter links to keep\n",
    "links_filtered = links[(links['from_node_id'].isin(valid_node_ids)) & (links['to_node_id'].isin(valid_node_ids))]\n",
    "\n",
    "links_filtered.to_csv('./data/2m/link.csv', index=False)\n",
    "\n",
    "print( \"Used POI and Connector are now stripped out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb65da-444b-49fd-8525-9cd895fab9c2",
   "metadata": {},
   "source": [
    "#### Relabelling nodes and links prioritizing closest node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f518913-bcd9-49d6-a92c-12126aae3c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link and Nodes have been repriotized\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the CSV files into pandas DataFrames\n",
    "node_df = pd.read_csv('./data/2m/node.csv',  low_memory=False)\n",
    "link_df = pd.read_csv('./data/2m/link.csv',  low_memory=False)\n",
    "zone_df = pd.read_csv('./data/2m/updated_zones_with_closest_node.csv', low_memory=False)\n",
    "\n",
    "# Exclude null values and identify closest nodes\n",
    "closest_nodes = zone_df['closest_node_id'].dropna().unique()\n",
    "\n",
    "# Ensure all nodes are sorted to maintain a predictable order after prioritizing closest nodes\n",
    "all_nodes_sorted = sorted(node_df['node_id'].unique())\n",
    "\n",
    "# New ordering: closest nodes first, followed by the rest, excluding already prioritized ones\n",
    "new_ordered_nodes = list(closest_nodes) + [node for node in all_nodes_sorted if node not in closest_nodes]\n",
    "\n",
    "# Create a mapping of old node IDs to new sequential IDs, starting from 1\n",
    "new_node_mapping = {old_id: new_id for new_id, old_id in enumerate(new_ordered_nodes, start=1)}\n",
    "\n",
    "# Apply the new labeling\n",
    "node_df['actual_node_id'] = node_df['node_id'].map(new_node_mapping)\n",
    "link_df['fromID'] = link_df['from_node_id'].map(new_node_mapping)\n",
    "link_df['toID'] = link_df['to_node_id'].map(new_node_mapping)\n",
    "\n",
    "# Save the updated DataFrames (optional)\n",
    "node_df.to_csv('./data/2m/node.csv', index=False)\n",
    "link_df.to_csv('./data/2m/link.csv', index=False)\n",
    "\n",
    "print( \"Link and Nodes have been repriotized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da78c2ff-1ad4-4a0d-a6fd-9bcc7dae08f6",
   "metadata": {},
   "source": [
    "#### Relabelling the demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "049be1bf-601f-420d-b581-755a47c59f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The demand has been relabeled\n"
     ]
    }
   ],
   "source": [
    "# Load the data from a CSV file\n",
    "df = pd.read_csv('./data/2m/demand.csv')\n",
    "\n",
    "# Creating a unique and sorted list of o_zone_id values\n",
    "sorted_unique_o_zones = sorted(pd.unique(df['d_zone_id']))\n",
    "mapping = {old_id: new_id for new_id, old_id in enumerate(sorted_unique_o_zones, start=1)}\n",
    "\n",
    "# Apply the mapping to create new columns for o_zone_id and d_zone_id\n",
    "df['o_zone_new_id'] = df['o_zone_id'].map(mapping)\n",
    "df['d_zone_new_id'] = df['d_zone_id'].map(mapping)\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file if needed\n",
    "# Change 'yourpath' to a directory you have access to\n",
    "df.to_csv('./data/2m/demand_modified.csv', index=False)\n",
    "\n",
    "print( \"The demand has been relabeled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6448993-90a2-4b1e-ba7a-9c2775d7a2d0",
   "metadata": {},
   "source": [
    "#### Calculating the capacity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e799f1b2-b8f2-4e11-995d-6f7e0f4db12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Capacity have been calculated. Move to the next file to generate the tntp file\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./data/2m/link.csv', low_memory=False)\n",
    "\n",
    "# Only perform the operation where the road type is not 'poi'\n",
    "data.loc[data['Road Type'] != 'poi', 'capacity'] = data['capacity'] * data['Number of Lanes']\n",
    "data.loc[data['Road Type'] == 'connector', 'capacity'] = 9999\n",
    "data.to_csv('./data/2m/link.csv', index=False)\n",
    "\n",
    "print( \"The Capacity have been calculated. Move to the next file to generate the tntp file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c14a28-2fa9-4d99-925f-a9ae87301553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geo)",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
